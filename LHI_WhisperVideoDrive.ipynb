{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LifeHackInnovationsLLC/whisper-video-transcription/blob/main/LHI_WhisperVideoDrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "857e207c",
        "lines_to_next_cell": 0,
        "ExecuteTime": {
          "end_time": "2024-12-19T19:47:20.818961Z",
          "start_time": "2024-12-19T19:47:20.816042Z"
        }
      },
      "source": [
        "# LHI_WhisperVideoDrive.py"
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f90aaf4f",
        "ExecuteTime": {
          "end_time": "2024-12-19T19:47:20.842788Z",
          "start_time": "2024-12-19T19:47:20.840843Z"
        }
      },
      "source": [
        "# ---\n",
        "# jupyter:\n",
        "#   jupytext:\n",
        "#     formats: ipynb,py:percent\n",
        "#     text_representation:\n",
        "#       extension: .py\n",
        "#       format_name: percent\n",
        "#       format_version: '1.3'\n",
        "#       jupytext_version: 1.16.5\n",
        "#   kernelspec:\n",
        "#     display_name: Python 3\n",
        "#     name: python3\n",
        "# ---"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LifeHackInnovationsLLC/whisper-video-transcription/blob/main/LHI_WhisperVideoDrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f7b404b",
        "lines_to_next_cell": 2
      },
      "source": [
        "### Jupytext Initialization (Sync Logic)\n",
        "Ensure Jupytext is installed and the notebook is paired with the `.py` file.\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def ensure_module(module_name, install_name=None):\n",
        "    \"\"\"Install a module if it's not already installed.\"\"\"\n",
        "    try:\n",
        "        __import__(module_name)\n",
        "        print(f\"Module '{module_name}' is already installed.\")\n",
        "    except ImportError:\n",
        "        install_name = install_name or module_name\n",
        "        print(f\"Module '{module_name}' not found. Installing...\")\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", install_name], check=True)\n",
        "\n",
        "Ensure Jupytext is installed\n",
        "ensure_module(\"jupytext\")\n",
        "\n",
        "Sync the notebook with its paired `.py` file\n",
        "try:\n",
        "    subprocess.run([\"jupytext\", \"--sync\", \"LHI_WhisperVideoDrive.ipynb\"], check=True)\n",
        "    print(\"Jupytext synchronization successful.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error during Jupytext synchronization: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e425e731",
        "outputId": "f88aaf91-3ad8-4d90-ddd2-a0e0cd373953",
        "title": "Ensure required modules are installed and imported",
        "ExecuteTime": {
          "end_time": "2024-12-19T19:47:22.764736Z",
          "start_time": "2024-12-19T19:47:20.852652Z"
        }
      },
      "source": [
        "# Handle missing modules and Google Colab environment checks\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "\n",
        "# Install and import required modules\n",
        "required_modules = {\n",
        "    \"google.colab\": \"google-colab\",\n",
        "    \"whisper\": \"openai-whisper\",\n",
        "    \"librosa\": \"librosa\",\n",
        "    \"soundfile\": \"soundfile\",\n",
        "    \"colorama\": \"colorama\",\n",
        "    \"google-api-python-client\": \"google-api-python-client\",\n",
        "    \"google-auth-httplib2\": \"google-auth-httplib2\",\n",
        "    \"google-auth-oauthlib\": \"google-auth-oauthlib\"\n",
        "}\n",
        "\n",
        "\n",
        "for module, install_name in required_modules.items():\n",
        "    try:\n",
        "        __import__(module)\n",
        "        print(f\"Module '{module}' is already installed.\")\n",
        "    except ImportError:\n",
        "        print(f\"Module '{module}' not found. Installing...\")\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", install_name], check=True)\n",
        "\n",
        "# Conditional import for Google Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"Google Colab environment detected.\")\n",
        "except ImportError:\n",
        "    print(\"Google Colab environment not detected. Skipping Colab imports.\")\n",
        "\n",
        "# Import other required modules\n",
        "import whisper\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module 'google.colab' is already installed.\n",
            "Module 'whisper' not found. Installing...\n",
            "Module 'librosa' is already installed.\n",
            "Module 'soundfile' is already installed.\n",
            "Module 'colorama' not found. Installing...\n",
            "Module 'google-api-python-client' not found. Installing...\n",
            "Module 'google-auth-httplib2' not found. Installing...\n",
            "Module 'google-auth-oauthlib' not found. Installing...\n",
            "Google Colab environment detected.\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z6Z6Z6ePz1R"
      },
      "source": [
        "\n",
        "#📼 OpenAI Whisper + Google Drive Video Transcription\n",
        "\n",
        "📺 Getting started video: https://youtu.be/YGpYinji7II\n",
        "\n",
        "###This application will extract audio from all the video files in a Google Drive folder and create a high-quality transcription with OpenAI's Whisper automatic speech recognition system.\n",
        "\n",
        "*Note: This requires giving the application permission to connect to your drive. Only you will have access to the contents of your drive, but please read the warnings carefully.*\n",
        "\n",
        "This notebook application:\n",
        "1. Connects to your Google Drive when you give it permission.\n",
        "2. Creates a WhisperVideo folder and three subfolders (ProcessedVideo, AudioFiles and TextFiles.)\n",
        "3. When you run the application it will search for all the video files (.mp4, .mov, mkv and .avi) in your WhisperVideo folder, transcribe them and then move the file to WhisperVideo/ProcessedVideo and save the transcripts to WhisperVideo/TextFiles. It will also add a copy of the new audio file to WhisperVideo/AudioFiles\n",
        "\n",
        "###**For faster performance set your runtime to \"GPU\"**\n",
        "*Click on \"Runtime\" in the menu and click \"Change runtime type\". Select \"GPU\".*\n",
        "\n",
        "\n",
        "**Note: If you add a new file after running this application you'll need to remount the drive in step 1 to make them searchable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhCHnaeTYw8"
      },
      "source": [
        "##0. Choose which 'LHI Client' or folder to add transcriptions to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L20Y96kiPz1R",
        "lines_to_next_cell": 2,
        "outputId": "00edec14-2e76-48fc-9597-fff7e91926f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Google Drive API using OAuth (User Credentials)...\n",
            "Google Drive API service initialized successfully as the user.\n",
            "Checking /content/drive status...\n",
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Drive is mounted and ready.\n",
            "Proceeding...\n",
            "Initializing Google Drive API using OAuth (User Credentials)...\n",
            "Google Drive API service initialized successfully as the user.\n",
            "Select a client folder:\n",
            "1: WCBradley\n",
            "2: SiriusXM\n",
            "3: LHI\n",
            "4: Enter a custom folder path\n",
            "Checking folder: /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/\n",
            "Folder already exists: /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/\n",
            "Checking folder: /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/AudioFiles/\n",
            "Folder already exists: /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/AudioFiles/\n",
            "Checking folder: /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/TextFiles/\n",
            "Folder already exists: /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/TextFiles/\n",
            "Checking folder: /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/ProcessedVideo/\n",
            "Folder already exists: /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/ProcessedVideo/\n",
            "WhisperVideo folder and subfolders initialized for client:\n",
            "WhisperVideo folder: /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/\n",
            "Audio files folder: /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/AudioFiles/\n",
            "Text files folder: /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/TextFiles/\n",
            "Processed videos folder: /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/ProcessedVideo/\n",
            "Folder 'Clients' found with ID: 1-ScYf1ZUli-IlejZbhFQPAHR3aBg_Tc5\n",
            "Folder 'WCBradley' found with ID: 17bG1M0aTeKQkELTrOQEAnXVX9U5wabFY\n",
            "Folder 'Videos' found with ID: 1hRUpMbXitI-Ms_actXqA8eiB0z3Nqrhv\n",
            "Folder 'WhisperVideo' found with ID: 10VXO4dTg36YySueayLgiAvzeC5dX5fNF\n",
            "Folder 'AudioFiles' found with ID: 1127EBNzrByWkFc47ShbHuXqoJTqRra4Z\n",
            "Folder 'TextFiles' found with ID: 11BSRsvsP_VcAlJBLK7CJf63s_9bIDLmO\n",
            "Folder 'ProcessedVideo' found with ID: 11Abuw26F1oF-kBp1LInveLmZ8OQYXI1b\n",
            "Folder 'AudioFiles' found with ID: 1127EBNzrByWkFc47ShbHuXqoJTqRra4Z\n",
            "Folder 'TextFiles' found with ID: 11BSRsvsP_VcAlJBLK7CJf63s_9bIDLmO\n",
            "Folder 'ProcessedVideo' found with ID: 11Abuw26F1oF-kBp1LInveLmZ8OQYXI1b\n",
            "=== REGISTRY TABLE ===\n",
            "╒════════╤════════════════╤══════════════════════════════════════════════════════════════════════════════╤═══════════════════════════════════╤══════════════════════════════════════════════════════════════════════════╕\n",
            "│ Type   │ Name           │ Path                                                                         │ ID                                │ URL                                                                      │\n",
            "╞════════╪════════════════╪══════════════════════════════════════════════════════════════════════════════╪═══════════════════════════════════╪══════════════════════════════════════════════════════════════════════════╡\n",
            "│ folder │ WhisperVideo   │ /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/                │ 10VXO4dTg36YySueayLgiAvzeC5dX5fNF │ https://drive.google.com/drive/folders/10VXO4dTg36YySueayLgiAvzeC5dX5fNF │\n",
            "├────────┼────────────────┼──────────────────────────────────────────────────────────────────────────────┼───────────────────────────────────┼──────────────────────────────────────────────────────────────────────────┤\n",
            "│ folder │ AudioFiles     │ /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/AudioFiles/     │ 1127EBNzrByWkFc47ShbHuXqoJTqRra4Z │ https://drive.google.com/drive/folders/1127EBNzrByWkFc47ShbHuXqoJTqRra4Z │\n",
            "├────────┼────────────────┼──────────────────────────────────────────────────────────────────────────────┼───────────────────────────────────┼──────────────────────────────────────────────────────────────────────────┤\n",
            "│ folder │ TextFiles      │ /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/TextFiles/      │ 11BSRsvsP_VcAlJBLK7CJf63s_9bIDLmO │ https://drive.google.com/drive/folders/11BSRsvsP_VcAlJBLK7CJf63s_9bIDLmO │\n",
            "├────────┼────────────────┼──────────────────────────────────────────────────────────────────────────────┼───────────────────────────────────┼──────────────────────────────────────────────────────────────────────────┤\n",
            "│ folder │ ProcessedVideo │ /content/drive/MyDrive/Clients/WCBradley/Videos/WhisperVideo/ProcessedVideo/ │ 11Abuw26F1oF-kBp1LInveLmZ8OQYXI1b │ https://drive.google.com/drive/folders/11Abuw26F1oF-kBp1LInveLmZ8OQYXI1b │\n",
            "╘════════╧════════════════╧══════════════════════════════════════════════════════════════════════════════╧═══════════════════════════════════╧══════════════════════════════════════════════════════════════════════════╛\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from colorama import Fore, Style, init\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "init(autoreset=True)\n",
        "\n",
        "# Global registry\n",
        "registry_entries = []\n",
        "\n",
        "def add_to_registry(entry_type, name, path, entity_id=None, is_file=False):\n",
        "    \"\"\"Add or update an entity in the registry.\"\"\"\n",
        "    url = None\n",
        "    if entity_id:\n",
        "        if is_file:\n",
        "            url = f\"https://drive.google.com/file/d/{entity_id}/view\"\n",
        "        else:\n",
        "            url = f\"https://drive.google.com/drive/folders/{entity_id}\"\n",
        "\n",
        "    # Update or add\n",
        "    for e in registry_entries:\n",
        "        if e[\"path\"] == path:\n",
        "            e[\"type\"] = entry_type\n",
        "            e[\"name\"] = name\n",
        "            e[\"id\"] = entity_id\n",
        "            e[\"url\"] = url if url else e[\"url\"]\n",
        "            return\n",
        "\n",
        "    registry_entries.append({\n",
        "        \"type\": entry_type,\n",
        "        \"name\": name,\n",
        "        \"path\": path,\n",
        "        \"id\": entity_id,\n",
        "        \"url\": url\n",
        "    })\n",
        "\n",
        "def remove_from_registry_by_path(path):\n",
        "    global registry_entries\n",
        "    registry_entries = [e for e in registry_entries if e[\"path\"] != path]\n",
        "\n",
        "def print_registry_table():\n",
        "    \"\"\"Print a table of all registered entries.\"\"\"\n",
        "    headers = [\"Type\", \"Name\", \"Path\", \"ID\", \"URL\"]\n",
        "    table_data = []\n",
        "    for e in registry_entries:\n",
        "        table_data.append([\n",
        "            e[\"type\"],\n",
        "            e[\"name\"],\n",
        "            e[\"path\"],\n",
        "            e[\"id\"] if e[\"id\"] else \"-\",\n",
        "            e[\"url\"] if e[\"url\"] else \"-\"\n",
        "        ])\n",
        "    print(Fore.CYAN + \"=== REGISTRY TABLE ===\")\n",
        "    print(tabulate(table_data, headers=headers, tablefmt=\"fancy_grid\"))\n",
        "\n",
        "def check_and_mount_drive():\n",
        "    print(\"Checking /content/drive status...\")\n",
        "    if os.path.exists(\"/content/drive\"):\n",
        "        print(\"Mount directory exists. Checking contents...\")\n",
        "        if os.listdir(\"/content/drive\"):\n",
        "            print(\"Mountpoint already contains files. Attempting to unmount...\")\n",
        "            print(\"Unmounted successfully or already unmounted.\")\n",
        "\n",
        "    # Mount Google Drive\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "    # Verify mount\n",
        "    if os.path.exists(\"/content/drive/MyDrive\"):\n",
        "        print(\"Drive is mounted and ready.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Mounting seems incomplete. Please check your drive configuration.\")\n",
        "        return False\n",
        "\n",
        "def initialize_drive_api():\n",
        "    \"\"\"\n",
        "    Initialize Google Drive API using OAuth user credentials.\n",
        "    This will prompt for user authentication.\n",
        "    \"\"\"\n",
        "    print(Fore.CYAN + \"Initializing Google Drive API using OAuth (User Credentials)...\")\n",
        "    try:\n",
        "        auth.authenticate_user()  # This will prompt you to authorize the app\n",
        "        service = build(\"drive\", \"v3\")\n",
        "        print(Fore.GREEN + \"Google Drive API service initialized successfully as the user.\")\n",
        "        return service\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"Failed to initialize Google Drive API: {e}\")\n",
        "        return None\n",
        "\n",
        "drive_service = initialize_drive_api()\n",
        "\n",
        "\n",
        "def get_file_id(file_name, folder_id):\n",
        "    \"\"\"\n",
        "    Retrieve the file ID for a given file name in a specific folder on Google Drive.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        results = drive_service.files().list(\n",
        "            q=f\"name='{file_name}' and '{folder_id}' in parents\",\n",
        "            spaces=\"drive\",\n",
        "            fields=\"files(id, name)\",\n",
        "            pageSize=1\n",
        "        ).execute()\n",
        "        items = results.get(\"files\", [])\n",
        "        if items:\n",
        "            return items[0][\"id\"]\n",
        "        else:\n",
        "            print(Fore.YELLOW + f\"File '{file_name}' not found in folder {folder_id}.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"Error retrieving file ID for '{file_name}': {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_shareable_link(file_id):\n",
        "    \"\"\"\n",
        "    Generate a shareable link for a given Google Drive file.\n",
        "    \"\"\"\n",
        "    print(Fore.CYAN + f\"Generating shareable link for file ID: {file_id}...\")\n",
        "    if drive_service is None:\n",
        "        print(Fore.RED + \"Drive service not initialized. Cannot generate link.\")\n",
        "        return None\n",
        "    try:\n",
        "        permission = {\"type\": \"anyone\", \"role\": \"reader\"}\n",
        "        drive_service.permissions().create(fileId=file_id, body=permission).execute()\n",
        "        link = f\"https://drive.google.com/file/d/{file_id}/view\"\n",
        "        print(Fore.GREEN + f\"Shareable link generated successfully: {link}\")\n",
        "        return link\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"Failed to generate shareable link: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_or_create_folder(drive_service, folder_name, parent_id):\n",
        "    \"\"\"\n",
        "    Retrieve or create a folder in Google Drive given a name and parent folder ID.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        query = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and '{parent_id}' in parents\"\n",
        "        results = drive_service.files().list(\n",
        "            q=query,\n",
        "            spaces=\"drive\",\n",
        "            fields=\"files(id, name)\",\n",
        "            pageSize=1\n",
        "        ).execute()\n",
        "        items = results.get(\"files\", [])\n",
        "\n",
        "        if items:\n",
        "            folder_id = items[0][\"id\"]\n",
        "            print(Fore.GREEN + f\"Folder '{folder_name}' found with ID: {folder_id}\")\n",
        "            return folder_id\n",
        "        else:\n",
        "            folder_metadata = {\n",
        "                \"name\": folder_name,\n",
        "                \"mimeType\": \"application/vnd.google-apps.folder\",\n",
        "                \"parents\": [parent_id]\n",
        "            }\n",
        "            folder = drive_service.files().create(body=folder_metadata, fields=\"id\").execute()\n",
        "            folder_id = folder.get(\"id\")\n",
        "            print(Fore.GREEN + f\"Folder '{folder_name}' created with ID: {folder_id}\")\n",
        "            return folder_id\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"Error creating or retrieving folder '{folder_name}': {e}\")\n",
        "        return None\n",
        "\n",
        "folder_id_cache = {}\n",
        "\n",
        "def get_folder_id_from_path(drive_service, local_path):\n",
        "    if local_path in folder_id_cache:\n",
        "        return folder_id_cache[local_path]\n",
        "\n",
        "    prefix = \"/content/drive/MyDrive/\"\n",
        "    if not local_path.startswith(prefix):\n",
        "        print(Fore.RED + \"The path does not start with /content/drive/MyDrive/.\")\n",
        "        return None\n",
        "\n",
        "    relative_path = local_path[len(prefix):].strip(\"/\")\n",
        "    if not relative_path:\n",
        "        folder_id_cache[local_path] = \"root\"\n",
        "        return \"root\"\n",
        "\n",
        "    parts = relative_path.split(\"/\")\n",
        "    current_parent_id = \"root\"\n",
        "    for part in parts:\n",
        "        folder_id = get_or_create_folder(drive_service, part, current_parent_id)\n",
        "        if not folder_id:\n",
        "            print(Fore.RED + f\"Failed to navigate/create the folder for part: {part}\")\n",
        "            return None\n",
        "        current_parent_id = folder_id\n",
        "\n",
        "    # Cache the final folder ID\n",
        "    folder_id_cache[local_path] = current_parent_id\n",
        "    return current_parent_id\n",
        "\n",
        "\n",
        "# Attempt to check and mount the drive\n",
        "if check_and_mount_drive():\n",
        "    print(\"Proceeding...\")\n",
        "else:\n",
        "    print(\"Drive mount failed. Exiting.\")\n",
        "    raise SystemExit(\"Drive mount failed.\")\n",
        "\n",
        "drive_service = initialize_drive_api()\n",
        "\n",
        "# Predefined options for client folders\n",
        "clients = {\n",
        "    \"1\": \"/content/drive/MyDrive/Clients/WCBradley/Videos/\",\n",
        "    \"2\": \"/content/drive/MyDrive/Clients/SiriusXM/Videos/\",\n",
        "    \"3\": \"/content/drive/MyDrive/Clients/LHI/Videos/\"\n",
        "}\n",
        "\n",
        "print(\"Select a client folder:\")\n",
        "print(\"1: WCBradley\")\n",
        "print(\"2: SiriusXM\")\n",
        "print(\"3: LHI\")\n",
        "print(\"4: Enter a custom folder path\")\n",
        "\n",
        "choice = input(\"Enter the number corresponding to your choice (default: 1): \").strip()\n",
        "if choice in clients:\n",
        "    client_videos_folder = clients[choice]\n",
        "elif choice == \"4\":\n",
        "    client_videos_folder = input(\"Enter the full path to your Videos folder: \").strip()\n",
        "else:\n",
        "    client_videos_folder = clients[\"1\"]\n",
        "\n",
        "rootFolder = client_videos_folder + \"WhisperVideo/\"\n",
        "audio_folder = rootFolder + \"AudioFiles/\"\n",
        "text_folder = rootFolder + \"TextFiles/\"\n",
        "processed_folder = rootFolder + \"ProcessedVideo/\"\n",
        "\n",
        "# Ensure local folders exist\n",
        "folders = [rootFolder, audio_folder, text_folder, processed_folder]\n",
        "for folder in folders:\n",
        "    try:\n",
        "        print(f\"Checking folder: {folder}\")\n",
        "        folder_name = os.path.basename(os.path.normpath(folder))\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "            print(Fore.GREEN + f\"Created folder: {folder}\")\n",
        "        else:\n",
        "            print(Fore.GREEN + f\"Folder already exists: {folder}\")\n",
        "        # Register locally. No ID yet.\n",
        "        add_to_registry(\"folder\", folder_name, folder)\n",
        "    except Exception as e:\n",
        "        print(Fore.RED + f\"Error ensuring folder {folder}: {e}\")\n",
        "\n",
        "print(Fore.CYAN + f\"WhisperVideo folder and subfolders initialized for client:\")\n",
        "print(Fore.GREEN + f\"WhisperVideo folder: {rootFolder}\")\n",
        "print(Fore.GREEN + f\"Audio files folder: {audio_folder}\")\n",
        "print(Fore.GREEN + f\"Text files folder: {text_folder}\")\n",
        "print(Fore.GREEN + f\"Processed videos folder: {processed_folder}\")\n",
        "\n",
        "# Now get or create these folders in Google Drive to get their IDs\n",
        "if drive_service:\n",
        "\n",
        "# Get folder names\n",
        "    audio_name = os.path.basename(os.path.normpath(audio_folder))\n",
        "    text_name = os.path.basename(os.path.normpath(text_folder))\n",
        "    processed_name = os.path.basename(os.path.normpath(processed_folder))\n",
        "\n",
        "    rootFolderID = get_folder_id_from_path(drive_service, rootFolder)\n",
        "    audio_folder_id = get_or_create_folder(drive_service, audio_name, rootFolderID)\n",
        "    text_folder_id = get_or_create_folder(drive_service, text_name, rootFolderID)\n",
        "    processed_folder_id = get_or_create_folder(drive_service, processed_name, rootFolderID)\n",
        "\n",
        "    if rootFolderID:\n",
        "        root_name = os.path.basename(os.path.normpath(rootFolder))\n",
        "        add_to_registry(\"folder\", root_name, rootFolder, rootFolderID, is_file=False)\n",
        "\n",
        "    audio_name = os.path.basename(os.path.normpath(audio_folder))\n",
        "    text_name = os.path.basename(os.path.normpath(text_folder))\n",
        "    processed_name = os.path.basename(os.path.normpath(processed_folder))\n",
        "\n",
        "    audio_id = get_or_create_folder(drive_service, audio_name, rootFolderID)\n",
        "    if audio_id:\n",
        "        add_to_registry(\"folder\", audio_name, audio_folder, audio_id, is_file=False)\n",
        "\n",
        "    text_id = get_or_create_folder(drive_service, text_name, rootFolderID)\n",
        "    if text_id:\n",
        "        add_to_registry(\"folder\", text_name, text_folder, text_id, is_file=False)\n",
        "\n",
        "    processed_id = get_or_create_folder(drive_service, processed_name, rootFolderID)\n",
        "    if processed_id:\n",
        "        add_to_registry(\"folder\", processed_name, processed_folder, processed_id, is_file=False)\n",
        "\n",
        "# Print the updated registry table with IDs and URLs\n",
        "print_registry_table()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFx0mfr031aw"
      },
      "source": [
        "##1. Load the code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PomTPiCR5ihc",
        "outputId": "81f7aa06-639a-428e-e39d-67542ea16390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-mmbsxxxs\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-mmbsxxxs\n",
            "  Resolved https://github.com/openai/whisper.git to commit 90db0de1896c23cbfaf0c58bc2d30665f709f170\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.8.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,196 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,632 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,448 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [53.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,614 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,517 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,517 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,830 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,564 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\n",
            "Fetched 28.1 MB in 4s (6,287 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.12.14)\n",
            "Requirement already satisfied: audioread in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:46<00:00, 3.12MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!sudo apt update && sudo apt install ffmpeg\n",
        "!pip install librosa\n",
        "!pip install audioread\n",
        "\n",
        "import whisper\n",
        "import time\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import re\n",
        "import os\n",
        "\n",
        "# model = whisper.load_model(\"tiny.en\")\n",
        "model = whisper.load_model(\"base.en\")\n",
        "# model = whisper.load_model(\"small.en\") # load the small model\n",
        "# model = whisper.load_model(\"medium.en\")\n",
        "# model = whisper.load_model(\"large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1CG2ZQerihnV"
      },
      "outputs": [],
      "source": [
        "# from colorama import Fore, Style, init\n",
        "# from googleapiclient.discovery import build\n",
        "# from google.oauth2.service_account import Credentials  # Ensure this import is included\n",
        "# from google.colab import drive\n",
        "\n",
        "# print(Fore.CYAN + \"Attempting to mount Google Drive...\")\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "# print(Fore.GREEN + \"Google Drive mounted successfully.\")\n",
        "\n",
        "# # Initialize colorama for console color support\n",
        "# init(autoreset=True)\n",
        "# print(Fore.CYAN + \"Colorama initialized for console color support.\")\n",
        "\n",
        "# # Google Drive API setup\n",
        "# def initialize_drive_api():\n",
        "#     \"\"\"\n",
        "#     Initialize Google Drive API service account for generating shareable links.\n",
        "#     \"\"\"\n",
        "#     print(Fore.CYAN + \"Initializing Google Drive API...\")\n",
        "#     try:\n",
        "#         credentials = Credentials.from_service_account_file(\n",
        "#             \"/content/drive/MyDrive/key.json\",\n",
        "#             scopes=[\"https://www.googleapis.com/auth/drive\"]\n",
        "#         )\n",
        "#         service = build(\"drive\", \"v3\", credentials=credentials)\n",
        "#         print(Fore.GREEN + \"Google Drive API service initialized successfully.\")\n",
        "#         return service\n",
        "#     except Exception as e:\n",
        "#         print(Fore.RED + f\"Failed to initialize Google Drive API: {e}\")\n",
        "#         return None\n",
        "\n",
        "# drive_service = initialize_drive_api()\n",
        "\n",
        "# def get_file_id(file_name, folder_id):\n",
        "#     \"\"\"\n",
        "#     Retrieve the file ID for a given file name in a specific folder on Google Drive.\n",
        "#     \"\"\"\n",
        "#     print(Fore.CYAN + f\"Searching for file '{file_name}' in folder ID '{folder_id}'...\")\n",
        "#     if drive_service is None:\n",
        "#         print(Fore.RED + \"Drive service not initialized. Cannot proceed.\")\n",
        "#         return None\n",
        "#     try:\n",
        "#         results = drive_service.files().list(\n",
        "#             q=f\"name='{file_name}' and '{folder_id}' in parents\",\n",
        "#             spaces=\"drive\",\n",
        "#             fields=\"files(id, name)\",\n",
        "#             pageSize=1\n",
        "#         ).execute()\n",
        "#         items = results.get(\"files\", [])\n",
        "#         if items:\n",
        "#             file_id = items[0][\"id\"]\n",
        "#             print(Fore.GREEN + f\"File '{file_name}' found with ID: {file_id}\")\n",
        "#             return file_id\n",
        "#         else:\n",
        "#             print(Fore.YELLOW + f\"File '{file_name}' not found in folder {folder_id}.\")\n",
        "#             return None\n",
        "#     except Exception as e:\n",
        "#         print(Fore.RED + f\"Error retrieving file ID for '{file_name}': {e}\")\n",
        "#         return None\n",
        "\n",
        "# def generate_shareable_link(file_id):\n",
        "#     \"\"\"\n",
        "#     Generate a shareable link for a given Google Drive file.\n",
        "#     \"\"\"\n",
        "#     print(Fore.CYAN + f\"Generating shareable link for file ID: {file_id}...\")\n",
        "#     if drive_service is None:\n",
        "#         print(Fore.RED + \"Drive service not initialized. Cannot generate link.\")\n",
        "#         return None\n",
        "#     try:\n",
        "#         permission = {\"type\": \"anyone\", \"role\": \"reader\"}\n",
        "#         drive_service.permissions().create(fileId=file_id, body=permission).execute()\n",
        "#         link = f\"https://drive.google.com/file/d/{file_id}/view\"\n",
        "#         print(Fore.GREEN + f\"Shareable link generated successfully: {link}\")\n",
        "#         return link\n",
        "#     except Exception as e:\n",
        "#         print(Fore.RED + f\"Failed to generate shareable link: {e}\")\n",
        "#         return None\n",
        "\n",
        "# # Example usage (uncomment to test):\n",
        "# # folder_id = \"YOUR_FOLDER_ID\"\n",
        "# # file_name = \"test.txt\"\n",
        "# # file_id = get_file_id(file_name, folder_id)\n",
        "# # if file_id:\n",
        "# #     link = generate_shareable_link(file_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIjETRxb5nuE"
      },
      "source": [
        "##2. Give the application permission to mount the drive and create the folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zxWvhDHzmspd",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# # Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\", force_remount=True)  # This will prompt for authorization.\n",
        "\n",
        "# import os\n",
        "\n",
        "# # Ensure WhisperVideo folder and its subfolders exist\n",
        "# folders = [rootFolder, audio_folder, text_folder, processed_folder]\n",
        "# for folder in folders:\n",
        "#     try:\n",
        "#         if not os.path.exists(folder):\n",
        "#             os.makedirs(folder)\n",
        "#             print(f\"Created folder: {folder}\")\n",
        "#         else:\n",
        "#             print(f\"Folder already exists: {folder}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error ensuring folder {folder}: {e}\")\n",
        "\n",
        "# print(f\"All folders verified and ready under: {rootFolder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "D_rB5M99nmhw",
        "lines_to_next_cell": 2,
        "outputId": "23f49e2d-4184-4f90-ed90-a5351ea645f6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hardware_accel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1a07246cd1df>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Hardware Accelerator Detected: {hardware_accel}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect_high_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hardware_accel' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import logging\n",
        "import csv\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import whisper\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "import sys\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# === Helper functions for metadata extraction ===\n",
        "\n",
        "def get_video_creation_date(video_path):\n",
        "    \"\"\"\n",
        "    Extract the creation date from video metadata using ffprobe.\n",
        "    If not available, fallback to the file's last modification time.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ffprobe command to extract creation_time\n",
        "        cmd = [\n",
        "            \"ffprobe\", \"-v\", \"quiet\", \"-print_format\", \"json\",\n",
        "            \"-show_entries\", \"format_tags=creation_time\",\n",
        "            video_path\n",
        "        ]\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            import json\n",
        "            metadata = json.loads(result.stdout)\n",
        "            creation_time = metadata.get(\"format\", {}).get(\"tags\", {}).get(\"creation_time\")\n",
        "            if creation_time:\n",
        "                # Parse the creation_time into a datetime object\n",
        "                # creation_time is often in ISO 8601 format\n",
        "                try:\n",
        "                    creation_date = datetime.fromisoformat(creation_time.replace(\"Z\",\"\"))\n",
        "                    return creation_date.isoformat(sep=' ', timespec='seconds')\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "        # Fallback to file modification time if creation_time not found\n",
        "        mod_time = os.path.getmtime(video_path)\n",
        "        return datetime.fromtimestamp(mod_time).isoformat(sep=' ', timespec='seconds')\n",
        "    except Exception as e:\n",
        "        # On any error, fallback to file modification time\n",
        "        mod_time = os.path.getmtime(video_path)\n",
        "        return datetime.fromtimestamp(mod_time).isoformat(sep=' ', timespec='seconds')\n",
        "\n",
        "# === Existing code starts ===\n",
        "\n",
        "def detect_hardware_accelerator():\n",
        "    \"\"\"Detect the hardware accelerator (GPU type) being used.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        return gpu_name\n",
        "    else:\n",
        "        # Check if TPU is available\n",
        "        try:\n",
        "            import tensorflow as tf\n",
        "            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "            return \"TPU\"\n",
        "        except ValueError:\n",
        "            return \"None\"\n",
        "\n",
        "def detect_high_ram():\n",
        "    \"\"\"Determine if the Colab environment is using high RAM.\"\"\"\n",
        "    try:\n",
        "        total_ram = subprocess.check_output(['grep', 'MemTotal', '/proc/meminfo']).decode('utf-8')\n",
        "        total_ram_kb = int(total_ram.split()[1])\n",
        "        total_ram_gb = total_ram_kb / (1024 ** 2)  # Convert KB to GB\n",
        "        if total_ram_gb >= 24:\n",
        "            return \"Yes\"\n",
        "        else:\n",
        "            return \"No\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error detecting high RAM: {e}\")\n",
        "        return \"Unknown\"\n",
        "\n",
        "def get_runtime_type():\n",
        "    \"\"\"Retrieve the runtime type information.\"\"\"\n",
        "    try:\n",
        "        python_version = sys.version.split()[0]  # e.g., '3.8.10'\n",
        "        runtime_info = f\"Python {python_version}\"\n",
        "        return runtime_info\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving runtime type: {e}\")\n",
        "        return \"Unknown\"\n",
        "\n",
        "def initialize_environment():\n",
        "    global hardware_accel, high_ram, runtime_type\n",
        "    hardware_accel = detect_hardware_accelerator()\n",
        "    high_ram = detect_high_ram()\n",
        "    runtime_type = get_runtime_type()\n",
        "    print(f\"Hardware Accelerator Detected: {hardware_accel}\")\n",
        "    print(f\"High RAM Enabled: {high_ram}\")\n",
        "    print(f\"Runtime Type: {runtime_type}\")\n",
        "\n",
        "initialize_environment()\n",
        "\n",
        "# Define your folder paths (ensure these match your initial setup)\n",
        "# These variables should be defined from your initial script environment\n",
        "# For demonstration, we assume they are defined elsewhere:\n",
        "# rootFolder, rootFolderID, text_name, processed_name, drive_service,\n",
        "# get_folder_id_from_path, get_or_create_folder, add_to_registry,\n",
        "# remove_from_registry_by_path, print_registry_table,\n",
        "# generate_shareable_link, get_file_id\n",
        "\n",
        "audio_folder = os.path.join(rootFolder, \"AudioFiles/\")\n",
        "text_folder = os.path.join(rootFolder, \"TextFiles/\")\n",
        "processed_folder = os.path.join(rootFolder, \"ProcessedVideo/\")\n",
        "\n",
        "registry_file_path = os.path.join(rootFolder, \"registry.json\")\n",
        "\n",
        "def load_registry():\n",
        "    \"\"\"Load the registry from a JSON file.\"\"\"\n",
        "    global registry_entries\n",
        "    if os.path.exists(registry_file_path):\n",
        "        with open(registry_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            registry_entries = json.load(f)\n",
        "        print(\"Registry loaded successfully.\")\n",
        "    else:\n",
        "        registry_entries = []\n",
        "        print(\"Registry file not found. Starting with an empty registry.\")\n",
        "\n",
        "def save_registry():\n",
        "    \"\"\"Save the registry to a JSON file.\"\"\"\n",
        "    with open(registry_file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(registry_entries, f, ensure_ascii=False, indent=4)\n",
        "    print(\"Registry saved successfully.\")\n",
        "\n",
        "try:\n",
        "    registry_entries\n",
        "except NameError:\n",
        "    load_registry()\n",
        "\n",
        "for folder in [rootFolder, audio_folder, text_folder, processed_folder]:\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "        print(f\"Created folder: {folder}\")\n",
        "    else:\n",
        "        print(f\"Folder exists: {folder}\")\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=os.path.join(rootFolder, \"processing_log.txt\"),\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        ")\n",
        "\n",
        "from datetime import timedelta\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"Convert seconds to HH:MM:SS format.\"\"\"\n",
        "    return str(timedelta(seconds=int(seconds)))\n",
        "\n",
        "def move_file_in_drive(drive_service, file_id, old_parent_id, new_parent_id):\n",
        "    \"\"\"\n",
        "    Move a file in Google Drive from old_parent_id to new_parent_id.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        file_info = drive_service.files().get(fileId=file_id, fields='parents').execute()\n",
        "        parents = file_info.get('parents', [])\n",
        "        if old_parent_id in parents:\n",
        "            parents.remove(old_parent_id)\n",
        "        updated_file = drive_service.files().update(\n",
        "            fileId=file_id,\n",
        "            addParents=new_parent_id,\n",
        "            removeParents=old_parent_id,\n",
        "            fields='id, parents'\n",
        "        ).execute()\n",
        "        print(f\"Moved file ID {file_id} from parent {old_parent_id} to {new_parent_id}.\")\n",
        "        return updated_file.get('id')\n",
        "    except Exception as e:\n",
        "        print(f\"Error moving file ID {file_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "text_folder_id = get_or_create_folder(drive_service, text_name, rootFolderID)\n",
        "processed_folder_id = get_or_create_folder(drive_service, processed_name, rootFolderID)\n",
        "\n",
        "if text_folder_id:\n",
        "    add_to_registry(\"folder\", text_name, text_folder, entity_id=text_folder_id, is_file=False)\n",
        "if processed_folder_id:\n",
        "    add_to_registry(\"folder\", processed_name, processed_folder, entity_id=processed_folder_id, is_file=False)\n",
        "\n",
        "def clear_old_files(folder, extension):\n",
        "    for f in os.listdir(folder):\n",
        "        if f.endswith(extension):\n",
        "            try:\n",
        "                os.remove(os.path.join(folder, f))\n",
        "                print(f\"Removed old file: {f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error removing file {f}: {e}\")\n",
        "\n",
        "clear_old_files(audio_folder, \".wav\")\n",
        "clear_old_files(text_folder, \".txt\")\n",
        "\n",
        "print(\"Initial Audio directory:\", os.listdir(audio_folder))\n",
        "print(\"Initial Text directory:\", os.listdir(text_folder))\n",
        "\n",
        "def file_in_registry_with_id(path):\n",
        "    return any(e[\"path\"] == path and e.get(\"id\") for e in registry_entries)\n",
        "\n",
        "def file_in_registry(path):\n",
        "    return any(e[\"path\"] == path for e in registry_entries)\n",
        "\n",
        "def get_file_bases(folder):\n",
        "    return {os.path.splitext(f)[0] for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))}\n",
        "\n",
        "def verify_folder_state():\n",
        "    videos = get_file_bases(processed_folder)\n",
        "    audios = get_file_bases(audio_folder)\n",
        "    texts = get_file_bases(text_folder)\n",
        "    all_match = (videos == audios == texts)\n",
        "    if not all_match:\n",
        "        print(\"WARNING: Folder parity mismatch detected:\")\n",
        "        print(f\"Processed Videos: {len(videos)} ({videos})\")\n",
        "        print(f\"Audio Files: {len(audios)} ({audios})\")\n",
        "        print(f\"Text Files: {len(texts)} ({texts})\")\n",
        "    return all_match\n",
        "\n",
        "def remove_duplicates(folder):\n",
        "    for fname in os.listdir(folder):\n",
        "        base, ext = os.path.splitext(fname)\n",
        "        if \"(1)\" in base:\n",
        "            try:\n",
        "                print(f\"Removing duplicate file: {fname}\")\n",
        "                os.remove(os.path.join(folder, fname))\n",
        "            except Exception as e:\n",
        "                print(f\"Error removing duplicate file {fname}: {e}\")\n",
        "\n",
        "def clean_incomplete_text_files(text_folder):\n",
        "    incomplete_files = []\n",
        "    for fname in os.listdir(text_folder):\n",
        "        if fname.lower().endswith(\".txt\"):\n",
        "            file_path = os.path.join(text_folder, fname)\n",
        "            try:\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    first_line = f.readline().strip()\n",
        "                    # Now the first line should be the divider. The second line should contain \"Original Video Link:\"\n",
        "                    # We can read the second line as well\n",
        "                    second_line = f.readline().strip()\n",
        "                    if not second_line.startswith(\"Original Video Link:\"):\n",
        "                        print(f\"Removing incomplete text file: {fname}\")\n",
        "                        os.remove(file_path)\n",
        "                        incomplete_files.append(fname)\n",
        "            except Exception as e:\n",
        "                print(f\"Error checking file {fname}: {e}\")\n",
        "                # Optionally, remove or handle the file\n",
        "    for fname in incomplete_files:\n",
        "        path = os.path.join(text_folder, fname)\n",
        "        remove_from_registry_by_path(path)\n",
        "    return incomplete_files\n",
        "\n",
        "def verify_and_cleanup_registry(text_folder, registry_entries):\n",
        "    txt_files = {f for f in os.listdir(text_folder) if f.lower().endswith(\".txt\")}\n",
        "    registry_txt_filenames = {os.path.basename(e[\"path\"]) for e in registry_entries if e[\"path\"].lower().endswith(\".txt\")}\n",
        "    extra_txt_files = txt_files - registry_txt_filenames\n",
        "    for fname in extra_txt_files:\n",
        "        file_path = os.path.join(text_folder, fname)\n",
        "        try:\n",
        "            print(f\"Removing extra text file not in registry: {fname}\")\n",
        "            os.remove(file_path)\n",
        "            logging.info(f\"Removed extra text file not in registry: {fname}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error removing extra text file {fname}: {e}\")\n",
        "    return extra_txt_files\n",
        "\n",
        "def register_and_upload_final_file(drive_service, entry_type, file_name, file_path, parent_folder_id, url=None):\n",
        "    if file_in_registry_with_id(file_path):\n",
        "        return None, None\n",
        "    else:\n",
        "        if file_in_registry(file_path):\n",
        "            remove_from_registry_by_path(file_path)\n",
        "        file_id = upload_file_to_drive(drive_service, file_path, parent_folder_id)\n",
        "        if file_id:\n",
        "            shareable_link = generate_shareable_link(file_id)\n",
        "            # Update registry with creation date if it's a text file associated with a video\n",
        "            # We'll handle this after we know what file we're dealing with in the main loop\n",
        "            add_to_registry(entry_type, file_name, file_path, entity_id=file_id, is_file=True, url=shareable_link)\n",
        "            return file_id, shareable_link\n",
        "        else:\n",
        "            return None, None\n",
        "\n",
        "def upload_file_to_drive(drive_service, file_path, parent_folder_id):\n",
        "    file_name = os.path.basename(file_path)\n",
        "    file_metadata = {\n",
        "        'name': file_name,\n",
        "        'parents': [parent_folder_id]\n",
        "    }\n",
        "    media = MediaFileUpload(file_path, resumable=True)\n",
        "    try:\n",
        "        file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
        "        print(f\"Uploaded file '{file_name}' with ID: {file.get('id')}\")\n",
        "        return file.get('id')\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading file '{file_name}': {e}\")\n",
        "        return None\n",
        "\n",
        "success_log = []\n",
        "error_log = []\n",
        "skipped_log = []\n",
        "\n",
        "video_files = [f for f in os.listdir(rootFolder) if os.path.isfile(os.path.join(rootFolder, f))]\n",
        "video_details = []\n",
        "\n",
        "for video_file in video_files:\n",
        "    if video_file == \"processing_report.txt\":\n",
        "        continue\n",
        "    if not video_file.lower().endswith((\".mp4\", \".mov\", \".avi\", \".mkv\")):\n",
        "        skipped_log.append((video_file, \"Invalid video format\"))\n",
        "        print(f\"Skipped {video_file}: Invalid video format.\")\n",
        "        continue\n",
        "\n",
        "    start_time = time.time()\n",
        "    runtime_type = \"Python\"\n",
        "    hardware_accel = \"None\"\n",
        "    high_ram = \"No\"\n",
        "    library_used = \"librosa\"\n",
        "    original_video_size_mb = os.path.getsize(os.path.join(rootFolder, video_file)) / (1024*1024)\n",
        "\n",
        "    base_name = os.path.splitext(video_file)[0]\n",
        "    video_path = os.path.join(rootFolder, video_file)\n",
        "    audio_path = os.path.join(audio_folder, base_name + \".wav\")\n",
        "    text_path = os.path.join(text_folder, base_name + \".txt\")\n",
        "    processed_path = os.path.join(processed_folder, video_file)\n",
        "\n",
        "    if os.path.exists(processed_path):\n",
        "        print(f\"Video {video_file} already processed. Skipping.\")\n",
        "        skipped_log.append((video_file, \"Already processed\"))\n",
        "        continue\n",
        "\n",
        "    if not file_in_registry(video_path):\n",
        "        add_to_registry(\"file\", video_file, video_path, entity_id=None, is_file=True)\n",
        "\n",
        "    video_id = get_file_id(video_file, rootFolderID)\n",
        "    if video_id:\n",
        "        remove_from_registry_by_path(video_path)\n",
        "        add_to_registry(\"file\", video_file, video_path, entity_id=video_id, is_file=True)\n",
        "\n",
        "    # Get creation date from video metadata\n",
        "    creation_date = get_video_creation_date(video_path)\n",
        "\n",
        "    print(f\"\\nProcessing {video_file}:\")\n",
        "    print(\"Audio directory:\", os.listdir(audio_folder))\n",
        "    print(\"Text directory:\", os.listdir(text_folder))\n",
        "\n",
        "    try:\n",
        "        need_ffmpeg = False\n",
        "        if not os.path.exists(audio_path):\n",
        "            print(f\"Extracting audio for {video_file} to {audio_path}\")\n",
        "            try:\n",
        "                y, sr = librosa.load(os.path.join(rootFolder, video_file), sr=16000)\n",
        "                sf.write(audio_path, y, sr)\n",
        "                print(f\"Audio extraction successful using librosa for {video_file}\")\n",
        "            except Exception as e_librosa:\n",
        "                print(f\"Librosa extraction failed for {video_file}: {e_librosa}. Using ffmpeg...\")\n",
        "                subprocess.run([\n",
        "                    \"ffmpeg\", \"-i\", os.path.join(rootFolder, video_file),\n",
        "                    \"-ar\", \"16000\", \"-ac\", \"1\", audio_path\n",
        "                ], check=True)\n",
        "                print(f\"Audio extraction successful using ffmpeg for {video_file}\")\n",
        "                need_ffmpeg = True\n",
        "        else:\n",
        "            print(f\"Audio file {audio_path} already exists.\")\n",
        "\n",
        "        if need_ffmpeg:\n",
        "            library_used = \"ffmpeg\"\n",
        "\n",
        "        print(f\"Uploading audio file {os.path.basename(audio_path)}...\")\n",
        "        audio_file_id, audio_link = register_and_upload_final_file(\n",
        "            drive_service, \"file\", os.path.basename(audio_path), audio_path, audio_folder_id, None\n",
        "        )\n",
        "\n",
        "        need_transcription = not os.path.exists(text_path)\n",
        "\n",
        "        if need_transcription:\n",
        "            print(f\"Starting transcription for {audio_path}\")\n",
        "            result = model.transcribe(audio_path)\n",
        "            print(f\"Transcription completed for {audio_path}\")\n",
        "\n",
        "            transcription_text = \"\"\n",
        "            for segment in result[\"segments\"]:\n",
        "                start_s = segment[\"start\"]\n",
        "                end_s = segment[\"end\"]\n",
        "                start_time_str = format_time(start_s)\n",
        "                end_time_str = format_time(end_s)\n",
        "                text_segment = segment[\"text\"].strip()\n",
        "                transcription_text += f\"[{start_time_str} - {end_time_str}] {text_segment}\\n\\n\"\n",
        "\n",
        "            temp_text_path = \"/tmp/\" + base_name + \".txt.tmp\"\n",
        "            print(f\"Saving transcription to temporary local file {temp_text_path}\")\n",
        "            with open(temp_text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(transcription_text)\n",
        "\n",
        "            print(f\"Generating shareable link for processed video {video_file}...\")\n",
        "            processed_video_link = generate_shareable_link(video_id) if video_id else \"\"\n",
        "\n",
        "            # Include the divider lines and the creation date at the top of the transcription\n",
        "            divider_line = \"------------------------------------------------------------\"\n",
        "            if processed_video_link:\n",
        "                final_transcription_content = (\n",
        "                    f\"{divider_line}\\n\"\n",
        "                    f\"Original Video Link: {processed_video_link}\\n\"\n",
        "                    f\"Original Creation Date: {creation_date}\\n\\n\"\n",
        "                    f\"{transcription_text}\"\n",
        "                    f\"{divider_line}\\n\"\n",
        "                )\n",
        "                print(f\"Saving final transcription file to {text_path}\")\n",
        "                with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(final_transcription_content)\n",
        "\n",
        "                print(f\"Uploading final text file {os.path.basename(text_path)}...\")\n",
        "                text_file_id, text_link = register_and_upload_final_file(\n",
        "                    drive_service, \"file\", os.path.basename(text_path), text_path, text_folder_id, None\n",
        "                )\n",
        "\n",
        "                # Update the registry entry for the text file to include the creation_date\n",
        "                # and similarly for the video\n",
        "                for e in registry_entries:\n",
        "                    if e[\"path\"] == text_path:\n",
        "                        e[\"creation_date\"] = creation_date\n",
        "                    if e[\"path\"] == video_path or e[\"path\"] == processed_path:\n",
        "                        e[\"creation_date\"] = creation_date\n",
        "\n",
        "                # Remove temporary file\n",
        "                os.remove(temp_text_path)\n",
        "                print(f\"Removed temporary local file {temp_text_path}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"No video ID found for {video_file}. Skipping link insertion.\")\n",
        "                # Without video link, just write transcription\n",
        "                final_transcription_content = (\n",
        "                    f\"{divider_line}\\n\"\n",
        "                    f\"Original Creation Date: {creation_date}\\n\\n\"\n",
        "                    f\"{transcription_text}\"\n",
        "                    f\"{divider_line}\\n\"\n",
        "                )\n",
        "                with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(final_transcription_content)\n",
        "                text_file_id, text_link = register_and_upload_final_file(\n",
        "                    drive_service, \"file\", os.path.basename(text_path), text_path, text_folder_id, None\n",
        "                )\n",
        "\n",
        "                # Update the registry entry to include creation_date\n",
        "                for e in registry_entries:\n",
        "                    if e[\"path\"] == text_path:\n",
        "                        e[\"creation_date\"] = creation_date\n",
        "                    if e[\"path\"] == video_path or e[\"path\"] == processed_path:\n",
        "                        e[\"creation_date\"] = creation_date\n",
        "        else:\n",
        "            print(f\"Text file {text_path} already exists, not retranscribing.\")\n",
        "            # Retrieve the existing text file's ID and URL from registry\n",
        "            existing_entry = next((e for e in registry_entries if e[\"path\"] == text_path), None)\n",
        "            text_file_id, text_link = (existing_entry[\"id\"], existing_entry.get(\"url\")) if existing_entry else (None, None)\n",
        "\n",
        "        print(f\"Moving file {video_file} to processed folder\")\n",
        "        shutil.move(os.path.join(rootFolder, video_file), processed_path)\n",
        "\n",
        "        if video_id:\n",
        "            move_file_in_drive(drive_service, video_id, rootFolderID, processed_folder_id)\n",
        "            remove_from_registry_by_path(video_path)\n",
        "            add_to_registry(\"file\", video_file, processed_path, entity_id=video_id, is_file=True)\n",
        "            # Update creation date in registry for processed video\n",
        "            for e in registry_entries:\n",
        "                if e[\"path\"] == processed_path:\n",
        "                    e[\"creation_date\"] = creation_date\n",
        "\n",
        "        print(\"Registry after processing this video:\")\n",
        "        print_registry_table()\n",
        "\n",
        "        if not verify_folder_state():\n",
        "            print(\"Folder parity mismatch after processing\", video_file)\n",
        "\n",
        "        end_time = time.time()\n",
        "        processing_time = end_time - start_time\n",
        "        efficiency = processing_time / original_video_size_mb if original_video_size_mb > 0 else \"\"\n",
        "\n",
        "        success_log.append(video_file)\n",
        "        logging.info(f\"Successfully processed {video_file}\")\n",
        "\n",
        "        registry_entry = next((e for e in registry_entries if e[\"path\"] == text_path and e.get(\"id\") == text_file_id), None)\n",
        "        video_details.append({\n",
        "            \"Name\": video_file,\n",
        "            \"Path\": processed_path,\n",
        "            \"Type\": \"video\",\n",
        "            \"ID\": video_id if video_id else \"\",\n",
        "            \"URL\": registry_entry[\"url\"] if registry_entry and registry_entry.get(\"url\") else \"\",\n",
        "            \"Status\": \"Processed\",\n",
        "            \"ProcessingTime\": processing_time,\n",
        "            \"RuntimeType\": runtime_type,\n",
        "            \"HardwareAccelerator\": hardware_accel,\n",
        "            \"HighRamUsed\": high_ram,\n",
        "            \"LibraryUsed\": library_used,\n",
        "            \"OriginalVideoSizeMB\": original_video_size_mb,\n",
        "            \"Efficiency\": efficiency\n",
        "        })\n",
        "\n",
        "    except subprocess.CalledProcessError as ffmpeg_error:\n",
        "        error_message = f\"FFmpeg error for {video_file}: {ffmpeg_error}\"\n",
        "        print(error_message)\n",
        "        error_log.append((video_file, error_message))\n",
        "        logging.error(error_message)\n",
        "\n",
        "    except Exception as general_error:\n",
        "        error_message = f\"General error for {video_file}: {general_error}\"\n",
        "        print(error_message)\n",
        "        error_log.append((video_file, error_message))\n",
        "        logging.error(error_message)\n",
        "\n",
        "remove_duplicates(audio_folder)\n",
        "remove_duplicates(text_folder)\n",
        "cleaned_files = clean_incomplete_text_files(text_folder)\n",
        "extra_cleaned_files = verify_and_cleanup_registry(text_folder, registry_entries)\n",
        "\n",
        "videos = get_file_bases(processed_folder)\n",
        "audios = get_file_bases(audio_folder)\n",
        "texts = get_file_bases(text_folder)\n",
        "all_match = (videos == audios == texts)\n",
        "\n",
        "report = \"Processing Report\\n\"\n",
        "report += f\"\\nSuccessfully Processed Files ({len(success_log)}):\\n\"\n",
        "report += \"\\n\".join(success_log)\n",
        "report += f\"\\n\\nSkipped Files ({len(skipped_log)}):\\n\"\n",
        "report += \"\\n\".join([f\"{file} - {reason}\" for file, reason in skipped_log])\n",
        "report += f\"\\n\\nErrors ({len(error_log)}):\\n\"\n",
        "report += \"\\n\".join([f\"{file} - {reason}\" for file, reason in error_log])\n",
        "report += f\"\\n\\nRemoved Incomplete Text Files ({len(cleaned_files)}):\\n\"\n",
        "report += \"\\n\".join(cleaned_files)\n",
        "report += f\"\\n\\nRemoved Extra Text Files Not in Registry ({len(extra_cleaned_files)}):\\n\"\n",
        "report += \"\\n\".join(extra_cleaned_files)\n",
        "report += f\"\\n\\nFolder Parity Check:\\n\"\n",
        "report += f\"All folders have matching files: {'Yes' if all_match else 'No'}\\n\"\n",
        "report += f\"Processed Videos: {len(videos)}\\n\"\n",
        "report += f\"Audio Files: {len(audios)}\\n\"\n",
        "report += f\"Text Files: {len(texts)}\\n\"\n",
        "\n",
        "with open(os.path.join(rootFolder, \"processing_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(\"=== COMPLETION REPORT ===\")\n",
        "print(report)\n",
        "\n",
        "csv_path = os.path.join(rootFolder, \"processing_log.csv\")\n",
        "file_exists = os.path.isfile(csv_path)\n",
        "current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "registry_data = []\n",
        "for e in registry_entries:\n",
        "    matching_video = next((vd for vd in video_details if vd[\"Name\"] == e[\"name\"] and vd[\"Path\"] == e[\"path\"]), None)\n",
        "    if matching_video:\n",
        "        row = {\n",
        "            \"Timestamp\": current_time,\n",
        "            \"Name\": e[\"name\"],\n",
        "            \"Path\": e[\"path\"],\n",
        "            \"ID\": e[\"id\"] if e.get(\"id\") else \"\",\n",
        "            \"URL\": e[\"url\"] if e.get(\"url\") else \"\",\n",
        "            \"Type\": matching_video[\"Type\"],\n",
        "            \"Status\": matching_video[\"Status\"],\n",
        "            \"ProcessingTime\": matching_video[\"ProcessingTime\"],\n",
        "            \"RuntimeType\": matching_video[\"RuntimeType\"],\n",
        "            \"HardwareAccelerator\": matching_video[\"HardwareAccelerator\"],\n",
        "            \"HighRamUsed\": matching_video[\"HighRamUsed\"],\n",
        "            \"LibraryUsed\": matching_video[\"LibraryUsed\"],\n",
        "            \"OriginalVideoSizeMB\": matching_video[\"OriginalVideoSizeMB\"],\n",
        "            \"Efficiency\": matching_video[\"Efficiency\"]\n",
        "        }\n",
        "    else:\n",
        "        entry_type = e[\"type\"]\n",
        "        row = {\n",
        "            \"Timestamp\": current_time,\n",
        "            \"Name\": e[\"name\"],\n",
        "            \"Path\": e[\"path\"],\n",
        "            \"ID\": e[\"id\"] if e.get(\"id\") else \"\",\n",
        "            \"URL\": e[\"url\"] if e.get(\"url\") else \"\",\n",
        "            \"Type\": entry_type,\n",
        "            \"Status\": \"\",\n",
        "            \"ProcessingTime\": \"\",\n",
        "            \"RuntimeType\": \"\",\n",
        "            \"HardwareAccelerator\": \"\",\n",
        "            \"HighRamUsed\": \"\",\n",
        "            \"LibraryUsed\": \"\",\n",
        "            \"OriginalVideoSizeMB\": \"\",\n",
        "            \"Efficiency\": \"\"\n",
        "        }\n",
        "    registry_data.append(row)\n",
        "\n",
        "fields = [\n",
        "    \"Timestamp\", \"Name\", \"Path\", \"ID\", \"URL\", \"Type\", \"Status\",\n",
        "    \"ProcessingTime\", \"RuntimeType\", \"HardwareAccelerator\", \"HighRamUsed\",\n",
        "    \"LibraryUsed\", \"OriginalVideoSizeMB\", \"Efficiency\"\n",
        "]\n",
        "\n",
        "with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
        "    if not file_exists:\n",
        "        writer.writeheader()\n",
        "    for row in registry_data:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(\"\\nCurrent CSV log entries:\")\n",
        "with open(csv_path, \"r\", encoding=\"utf-8\") as csvfile:\n",
        "    print(csvfile.read())\n",
        "\n",
        "save_registry()\n",
        "\n",
        "# === Function to merge all text files into one merged.txt after processing ===\n",
        "def merge_all_text_files(text_folder):\n",
        "    merged_file_path = os.path.join(text_folder, \"merged.txt\")\n",
        "    with open(merged_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        for fname in sorted(os.listdir(text_folder)):\n",
        "            if fname.lower().endswith(\".txt\") and fname != \"merged.txt\":\n",
        "                file_path = os.path.join(text_folder, fname)\n",
        "                # Write a header for each file's content\n",
        "                outfile.write(f\"\\n\\n=== {fname} ===\\n\\n\")\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
        "                    outfile.write(infile.read())\n",
        "    print(f\"Merged all text files into {merged_file_path}\")\n",
        "\n",
        "# Call the merge function after all processing is done\n",
        "merge_all_text_files(text_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fr8tBQy5Tvo"
      },
      "source": [
        "##3. Upload any video files you want transcribed in the \"WhisperVideo\" folder in your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCef9V2i392e"
      },
      "source": [
        "## 4. Extract audio from the video files and create a transcription\n",
        "\n",
        "This step processes video files in the `WhisperVideo` folder by extracting audio, transcribing it, and saving the transcription in the `TextFiles` folder. The original video file is moved to the `ProcessedVideo` folder upon successful transcription.\n",
        "\n",
        "### Shareable Links\n",
        "The shareable link for the processed video is generated based on its Google Drive file path. This method avoids additional API calls and assumes that files are already shared within your team. The constructed link can be found at the beginning of the transcription file.\n",
        "\n",
        "Example of a shareable link format:\n",
        "```\n",
        "https://drive.google.com/file/d/<file_id>/view\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCvv85Y_u8V7"
      },
      "outputs": [],
      "source": [
        "# ### Final Note for Synchronization\n",
        "# For Colab: Sync changes manually after downloading the notebook.\n",
        "# For Local: Use the Jupytext command:\n",
        "#    jupytext --sync LHI_WhisperVideoDrive.ipynb\n",
        "\n",
        "print(\"Final Note: Synchronize your files locally using Jupytext.\")\n",
        "print(\"Colab users: Save your notebook and download it to sync manually.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "formats": "ipynb,py:percent",
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}